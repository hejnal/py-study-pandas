{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/hejnal/py-study-pandas/blob/main/notebooks/PyStudy_Group_13_Exercise_2_BigFrames_ipynb_%5BMAKE_A_COPY%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_svDOh2oQRDI"
      },
      "source": [
        "PyStudy Group #13 - Exercise 2 - BigFrames\n",
        "The purpose of this notebook is practice BigFrames API and compare it with a classic Pandas development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VH038ecQ8Nb"
      },
      "source": [
        "## Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KcRZdcePDvf"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install -q --upgrade bigframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHHrJ_FDRabc"
      },
      "outputs": [],
      "source": [
        "# Load BigQuery Magic extension\n",
        "%load_ext google.cloud.bigquery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5MPdsHyRCCB"
      },
      "source": [
        "## Project Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ucECCuyssZcn"
      },
      "outputs": [],
      "source": [
        "# Importing BigFrames\n",
        "import bigframes.pandas as bpd\n",
        "# Importing the matplotlib library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PROJECT_ID = \"whejna-py-study\"  # @param {type:\"string\"}\n",
        "REGION = \"EU\"  # @param {type:\"string\"}\n",
        "SCENARIOS = {\n",
        "    \"SMALL_DATA\": 1000000,\n",
        "    \"MEDIUM_DATA\": 10000000,\n",
        "    \"BIG_DATA\": 100000000,\n",
        "}\n",
        "\n",
        "# Set the project ID\n",
        "bpd.options.bigquery.project = PROJECT_ID\n",
        "bpd.close_session()\n",
        "\n",
        "# Set BigQuery DataFrames options\n",
        "# Note: The project option is not required in all environments.\n",
        "# On BigQuery Studio, the project ID is automatically detected.\n",
        "bpd.options.bigquery.project = PROJECT_ID\n",
        "\n",
        "# Note: The location option is not required.\n",
        "# It defaults to the location of the first table or query\n",
        "# passed to read_gbq(). For APIs where a location can't be\n",
        "# auto-detected, the location defaults to the \"US\" location.\n",
        "bpd.options.bigquery.location = REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4GpfaeKROpx"
      },
      "source": [
        "## Authenticate the notebook (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJPzOkPcPUwC"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0_uw3Abbz-E"
      },
      "source": [
        "## Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "W9C2LMXJTEub"
      },
      "outputs": [],
      "source": [
        "# Create some utils methods\n",
        "from pandas import DataFrame\n",
        "\n",
        "def calculate_hourly_hires(df: DataFrame):\n",
        "  df['start_hour'] = df['start_date'].dt.hour\n",
        "  hourly_hires = df.groupby('start_hour').size()\n",
        "  return hourly_hires\n",
        "\n",
        "def visualize_hourly_hires(df: DataFrame):\n",
        "  plt.bar(df.index, df.values)\n",
        "  plt.xlabel('Hour of the Day')\n",
        "  plt.ylabel('Number of Hires')\n",
        "  plt.title('Number of Bicycle Hires by Hour of the Day')\n",
        "\n",
        "def visualize_elapsed_times(elapsed_times: dict):\n",
        "  labels = list(elapsed_times.keys())\n",
        "  values = list(elapsed_times.values())\n",
        "  colors = ['blue', 'green', 'red']\n",
        "\n",
        "  plt.bar(labels, values, color=colors)\n",
        "  plt.xlabel(\"Methods\")\n",
        "  plt.ylabel(\"Elapsed Time\")\n",
        "  plt.title(f\"Elapsed Time for Each Method, with the MAX_ROWS={MAX_ROWS}\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw6vEdV_e-vc"
      },
      "source": [
        "# Use Case 1 [Pandas]: Benchmark London Bicycles Hire: Analyze peak usage hours\n",
        "\n",
        "Using public dataset (located in the EU) Compare 3 different mechanisms to solve this problem:\n",
        "\n",
        "1. Pandas DataFrame\n",
        "2. BigQuery DataFrame\n",
        "3. BigQuery Plan SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_H4pWawcQia"
      },
      "source": [
        "## Set Scenario (SMALL_DATA, MEDIUM_DATA or BIG_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ypARthjRbu8I"
      },
      "outputs": [],
      "source": [
        "MAX_ROWS = SCENARIOS[\"SMALL_DATA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vTxbzfTpi4Oa"
      },
      "outputs": [],
      "source": [
        "# Some extra structures for measuring the elapsed times\n",
        "elapsed_times = {\n",
        "    \"pandas\": 0.0,\n",
        "    \"bigframes\": 0.0,\n",
        "    \"direct_bq_sql\": 0.0,\n",
        "}\n",
        "\n",
        "params = {\n",
        "  \"max_rows\": MAX_ROWS\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjGHozwlfGNl"
      },
      "source": [
        "## Method 1: Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA4qDQQ2PW25"
      },
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1 -o\n",
        "%%bigquery cycle_hire_df --params $params --project $PROJECT_ID --no_query_cache\n",
        "SELECT start_date, start_station_name, end_station_name, duration\n",
        "FROM\n",
        "`bigquery-public-data`.london_bicycles.cycle_hire\n",
        "WHERE duration > 0\n",
        "LIMIT @max_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KeikCBM4ilHH"
      },
      "outputs": [],
      "source": [
        "# capture the time for the last operation\n",
        "elapsed_times[\"pandas\"] += _.best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SI3WGN0LW7jk"
      },
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1 -o\n",
        "hourly_hires = calculate_hourly_hires(df=cycle_hire_df)\n",
        "visualize_hourly_hires(df=hourly_hires)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "i9DDDHjBjY9V"
      },
      "outputs": [],
      "source": [
        "# capture the time for the last operation\n",
        "elapsed_times[\"pandas\"] += _.best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii5Aj2tdfMYk"
      },
      "source": [
        "## Method 2: BigQuery DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHvI7auIVRvy"
      },
      "outputs": [],
      "source": [
        "import time # use time module, as the timeit runs in isolated scope and does not let to capture the output\n",
        "start_time = time.time()\n",
        "# Create a BigFrame from the cycle_hire table\n",
        "cycle_hire_bf = bpd.read_gbq(\n",
        "    \"bigquery-public-data.london_bicycles.cycle_hire\", max_results=MAX_ROWS, use_cache=False, columns=[\"start_date\", \"start_station_name\", \"end_station_name\", \"duration\"],  filters=[(\"duration\", \">\", 0)]\n",
        ")\n",
        "end_time = time.time()\n",
        "elapsed = end_time - start_time\n",
        "elapsed_times[\"bigframes\"] += elapsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZS76KGbV6lG"
      },
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1 -o\n",
        "hourly_hires_bf = calculate_hourly_hires(df=cycle_hire_bf)\n",
        "visualize_hourly_hires(df=hourly_hires_bf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1oEA4eZp1Ch"
      },
      "outputs": [],
      "source": [
        "# capture the time for the last operation\n",
        "elapsed_times[\"bigframes\"] += _.best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smP4a-e_g4LZ"
      },
      "source": [
        "## Method 3 Query in BigQuery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sCDXMTlg6WY"
      },
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1 -o\n",
        "%%bigquery peak_time --params $params --project $PROJECT_ID --no_query_cache\n",
        "WITH base_with_limit AS (\n",
        "  SELECT start_date\n",
        "  FROM\n",
        "    `bigquery-public-data`.london_bicycles.cycle_hire\n",
        "  WHERE duration > 0\n",
        "  LIMIT @max_rows\n",
        ")\n",
        "SELECT\n",
        "  EXTRACT(HOUR FROM start_date) AS start_hour,\n",
        "  COUNT(*) AS num_hires\n",
        "FROM\n",
        "    base_with_limit\n",
        "GROUP BY 1\n",
        "ORDER BY 1 ASC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMfjySKZp7UX"
      },
      "outputs": [],
      "source": [
        "# capture the time for the last operation\n",
        "elapsed_times[\"direct_bq_sql\"] += _.best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQfy2bvthPXD"
      },
      "outputs": [],
      "source": [
        "%%timeit -n1 -r1 -o\n",
        "visualize_hourly_hires(df=peak_time[\"num_hires\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QD7-TOVylKW"
      },
      "outputs": [],
      "source": [
        "# capture the time for the last operation\n",
        "elapsed_times[\"direct_bq_sql\"] += _.best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN1_geTgy6Ij"
      },
      "source": [
        "## Compare the 3 different methods for a given batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ3tAwNaymb3"
      },
      "outputs": [],
      "source": [
        "# SMALL DATA\n",
        "visualize_elapsed_times(elapsed_times=elapsed_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8HdGKsCaFb_"
      },
      "source": [
        "## TODO: Exercise 1 Run Full Benchmark\n",
        "Plot elapsed times for 2 other scenarios: MEDIUM_DATA and BIG_DATA\n",
        "\n",
        "**Hint**: set MAX_ROWS to different scenarios at the beginning of the notebook and re-run the whole Scenario 1 block.\n",
        "\n",
        "Scroll to the [Scenario 1 block](#scrollTo=v_H4pWawcQia) cell to set-up the new scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHWiEwp10QmR"
      },
      "outputs": [],
      "source": [
        "# Scenario: MEDIUM_DATA\n",
        "visualize_elapsed_times(elapsed_times=elapsed_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdO4xbcvGQM7"
      },
      "outputs": [],
      "source": [
        "# Scenario: BIG_DATA\n",
        "visualize_elapsed_times(elapsed_times=elapsed_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C5CrNFZJTwc"
      },
      "outputs": [],
      "source": [
        "# Convert to Pandas DataFrame\n",
        "pd_df = average_duration_by_station.head(10).to_pandas()\n",
        "\n",
        "# Plot using Pandas\n",
        "plt.figure(figsize=(10, 6))\n",
        "pd_df.plot(kind=\"bar\")\n",
        "plt.title(\"Top 10 Stations with Longest Average Rental Duration\")\n",
        "plt.xlabel(\"Station Name\")\n",
        "plt.ylabel(\"Average Duration (seconds)\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5U7zrOXlnlK"
      },
      "source": [
        "# Use Case 2 [Pandas]: Get Monthly Rental Trends and Most Popular Stations With BigFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj0Qm1bw3_Bf"
      },
      "outputs": [],
      "source": [
        "MAX_ROWS = SCENARIOS[\"SMALL_DATA\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1qVIAZzi32a"
      },
      "outputs": [],
      "source": [
        "# Create BigFrames from the tables (hires and stations)\n",
        "cycle_hire_bfd = bpd.read_gbq(\"bigquery-public-data.london_bicycles.cycle_hire\", max_results=MAX_ROWS, use_cache=False, filters=[(\"duration\", \">\", 0)])\n",
        "cycle_stations_bfd = bpd.read_gbq(\"bigquery-public-data.london_bicycles.cycle_stations\", use_cache=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqw3y575P2Zz"
      },
      "source": [
        "## TODO: Exercise 2: Get Monthly Rental Trends"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08z8eHCRKUsf"
      },
      "outputs": [],
      "source": [
        "# Calculate the average duration of rentals for each station\n",
        "average_duration_by_station = # TODO: group by start_station_name and calculate mean of duration\n",
        "\n",
        "# Display the top 10 stations with the longest average rental durations\n",
        "print(average_duration_by_station.head(10))\n",
        "\n",
        "# Analyze rental trends over time\n",
        "# Extract the month from a date\n",
        "cycle_hire_bfd[\"month\"] = cycle_hire_bfd[\"start_date\"].dt.month\n",
        "\n",
        "# Group by month, and get the average duration\n",
        "monthly_rentals = # TODO: group by month and get avg duration\n",
        "\n",
        "# Convert the Series to a DataFrame for easier plotting\n",
        "monthly_rentals = monthly_rentals.reset_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "plt.plot(\n",
        "    monthly_rentals[\"month\"],\n",
        "    monthly_rentals[\"avg_duration\"],\n",
        "    marker=\"o\",\n",
        "    linestyle=\"-\",\n",
        ")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Average Rental Duration\")\n",
        "plt.title(\"Trend of Average Rental Duration Over Time\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "b9R0cxq0K5a5"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "\n",
        "# Calculate the average duration of rentals for each station\n",
        "average_duration_by_station = (\n",
        "    cycle_hire_bfd.groupby(\"start_station_name\")[\"duration\"].mean().sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "# Display the top 10 stations with the longest average rental durations\n",
        "print(average_duration_by_station.head(10))\n",
        "\n",
        "# Analyze rental trends over time\n",
        "# Extract the month from a date\n",
        "cycle_hire_bfd[\"month\"] = cycle_hire_bfd[\"start_date\"].dt.month\n",
        "\n",
        "# Group by month, and get the average duration\n",
        "monthly_rentals = cycle_hire_bfd.groupby([\"month\"]).agg(\n",
        "    avg_duration=(\"duration\", \"mean\")\n",
        ")\n",
        "\n",
        "# Convert the Series to a DataFrame for easier plotting\n",
        "monthly_rentals = monthly_rentals.reset_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "plt.plot(\n",
        "    monthly_rentals[\"month\"],\n",
        "    monthly_rentals[\"avg_duration\"],\n",
        "    marker=\"o\",\n",
        "    linestyle=\"-\",\n",
        ")\n",
        "plt.xlabel(\"Month\")\n",
        "plt.ylabel(\"Average Rental Duration\")\n",
        "plt.title(\"Trend of Average Rental Duration Over Time\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj2il8273lKc"
      },
      "source": [
        "## TODO: Exercise 3: Plot on the map 100 most busy stations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvxCPlm461pO"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas shapely geodatasets matplotlib -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RR2_zQGnRsp"
      },
      "outputs": [],
      "source": [
        "!wget -P /content/sample_data/ https://github.com/gicentre/data/blob/03ad5e41d3023c48c047bff9dffc574933c07a67/geoTutorials/London_Borough_Excluding_MHW.shp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLzSvAqDKIPs"
      },
      "source": [
        "For this exercise use pandas DF, it is much faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KixwNSNuBY5b"
      },
      "outputs": [],
      "source": [
        "cycle_hire_df = cycle_hire_bf.to_pandas()\n",
        "cycle_stations_df = cycle_stations_bf.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SMHObp6QIz8"
      },
      "source": [
        "TODO: Fix the following aggregations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AABbVXZVQQ5Z"
      },
      "outputs": [],
      "source": [
        "joined_df = # TODO: Hint use merge operation between cycle_hire_df and cycle_stations_df\n",
        "\n",
        "most_popular_stations = # TODO: first use filter (start_station_name, longitude, latitude), then group by the station name and then apply few aggregations (size, first of longitude and latitude)\n",
        "\n",
        "top_n_stations = # TODO: get top 100 stations\n",
        "bottom_n_stations = # TODO: get bottom 100 stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Hc65WDZt8mk8"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "joined_df = cycle_hire_df.merge(cycle_stations_df, left_on=\"start_station_name\", right_on=\"name\", how=\"inner\")\n",
        "\n",
        "most_popular_stations = joined_df.filter(items=[\"start_station_name\", \"longitude\", \"latitude\"]).groupby(\"start_station_name\").agg(\n",
        "    count=(\"start_station_name\", \"size\"), # Count occurrences\n",
        "    longitude=(\"longitude\", \"first\"), # Take first longitude\n",
        "    latitude=(\"latitude\", \"first\")  # Take first latitude\n",
        ").sort_values(by=\"count\", ascending=False)\n",
        "\n",
        "top_n_stations = most_popular_stations.head(100)\n",
        "bottom_n_stations = most_popular_stations.tail(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKkbkGchRHkr"
      },
      "source": [
        "Define some helper methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t8vdSgi5bj2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from pyogrio import set_gdal_config_options\n",
        "\n",
        "set_gdal_config_options({\n",
        "    'SHAPE_RESTORE_SHX': 'YES',\n",
        "  })\n",
        "\n",
        "def visualize_station_geo(top_df: pd.DataFrame, bottom_df: pd.DataFrame):\n",
        "\n",
        "  london = gpd.read_file('/content/sample_data/London_Borough_Excluding_MHW.shp')\n",
        "  london = london.set_crs(\"epsg:27700\")\n",
        "\n",
        "  # Use the new syntax here\n",
        "  london['geometry'] = london['geometry'].to_crs('epsg:4326')\n",
        "\n",
        "  top_gdf = gpd.GeoDataFrame(top_df, crs = 'epsg:4326', geometry = gpd.points_from_xy(top_df['longitude'], top_df['latitude']))\n",
        "  bottom_gdf = gpd.GeoDataFrame(bottom_df, crs = 'epsg:4326', geometry = gpd.points_from_xy(bottom_df['longitude'], bottom_df['latitude']))\n",
        "\n",
        "  ax = london.plot(figsize=(30, 12))\n",
        "  top_gdf.plot(ax=ax, marker='o', color='yellow', markersize=30)\n",
        "  bottom_gdf.plot(ax=ax, marker='o', color='red', markersize=30)\n",
        "\n",
        "  ax.set_xlim(london.total_bounds[0], london.total_bounds[2])\n",
        "  ax.set_ylim(london.total_bounds[1], london.total_bounds[3])\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz_J9zx1RKZx"
      },
      "source": [
        "Visualize the stations on the London map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNlNssr56MAZ"
      },
      "outputs": [],
      "source": [
        "visualize_station_geo(top_df=top_n_stations, bottom_df=bottom_n_stations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOWKkV14SjjS"
      },
      "source": [
        "# Use Case 3 [Pandas]: Custom UDFs via Remote Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFA4FZmzAGUq"
      },
      "source": [
        "Enable APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4mwBsOiHPb4"
      },
      "outputs": [],
      "source": [
        "!gcloud --project $PROJECT_ID services enable bigqueryconnection.googleapis.com cloudfunctions.googleapis.com run.googleapis.com cloudbuild.googleapis.com artifactregistry.googleapis.com cloudresourcemanager.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk4lcIsnm9E0"
      },
      "source": [
        "Consult the help around remote functions in BigFrames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jbJyEZYC6Fw"
      },
      "outputs": [],
      "source": [
        "help(bpd.remote_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTCH-K-PSggf"
      },
      "outputs": [],
      "source": [
        "import bigframes.pandas as bpd\n",
        "\n",
        "# Load the dataframe\n",
        "cycle_hire_bf = bpd.read_gbq(\n",
        "    \"bigquery-public-data.london_bicycles.cycle_hire\", max_results=MAX_ROWS, use_cache=False, columns=[\"duration\"],  filters=[(\"duration\", \">\", 0)]\n",
        ")\n",
        "\n",
        "# Define a remote function to categorize rental duration\n",
        "@bpd.remote_function(\n",
        "    int,  # Input type (duration in seconds)\n",
        "    str,  # Output type (category)\n",
        "    reuse=False,\n",
        ")\n",
        "def categorize_duration(duration_sec: int) -> str:\n",
        "    if duration_sec <= 300:  # 5 minutes\n",
        "        return \"Short Trip\"\n",
        "    elif duration_sec <= 1800:  # 30 minutes\n",
        "        return \"Medium Trip\"\n",
        "    else:\n",
        "        return \"Long Trip\"\n",
        "\n",
        "# Apply the remote function to create a new column\n",
        "cycle_hire_bf = cycle_hire_bf.assign(\n",
        "    trip_category=cycle_hire_bf[\"duration\"].apply(categorize_duration)\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "print(cycle_hire_bf[[\"duration\", \"trip_category\"]].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwgd-Q14vk-e"
      },
      "source": [
        "# Use Case 4 [ML]: Predict Rental Duration with BigFrames ML and Scikit Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20KIo8fBAqMy"
      },
      "source": [
        "Load weather data from public dataset (load to pandas, as they are in a different region). Only use data for 2022 and a single London Weather Station (HEATHROW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zupEraHLvqsJ"
      },
      "outputs": [],
      "source": [
        "%%bigquery weather_data_df --project $PROJECT_ID\n",
        "SELECT\n",
        "  date, value as precipitation\n",
        "FROM\n",
        "  `bigquery-public-data.ghcn_d.ghcnd_2022` AS wx\n",
        "WHERE\n",
        "  date >= \"2022-01-01\"\n",
        "  AND date <= \"2022-12-31\"\n",
        "  AND id = 'UKM00003772'\n",
        "  AND qflag IS NULL\n",
        "  AND element = 'PRCP'\n",
        "ORDER BY\n",
        "  wx.date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irPj7QC1ThcB"
      },
      "source": [
        "Prepare the features for ML modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqGceSJHTgl3"
      },
      "outputs": [],
      "source": [
        "MAX_ROWS = SCENARIOS[\"MEDIUM_DATA\"]    # Load full dataset for ML\n",
        "\n",
        "# 1. Load the bike rentals data from BigQuery and prepare the features\n",
        "# Load the dataframe\n",
        "cycle_hire_bdf = bpd.read_gbq(\n",
        "    \"bigquery-public-data.london_bicycles.cycle_hire\", max_results=MAX_ROWS, use_cache=False, columns=[\"start_date\", \"start_station_name\", \"duration\"],  filters=[(\"duration\", \">\", 0)]\n",
        ")\n",
        "\n",
        "cycle_hire_df = cycle_hire_bdf.to_pandas()   # cannot join between US and EU, so use both dataframes in pandas, pay attention to the LOAD jobs\n",
        "\n",
        "cycle_hire_df['date'] = cycle_hire_df['start_date'].dt.date\n",
        "cycle_hire_df['hour'] = cycle_hire_df['start_date'].dt.hour\n",
        "cycle_hire_df['dayofweek'] = cycle_hire_df['start_date'].dt.dayofweek\n",
        "cycle_hire_df['month'] = cycle_hire_df['start_date'].dt.month\n",
        "cycle_hire_df['year'] = cycle_hire_df['start_date'].dt.year\n",
        "\n",
        "filtered_df = cycle_hire_df.loc[:, [\"year\", \"date\", \"dayofweek\", \"start_station_name\"]].loc[cycle_hire_df[\"year\"] == 2022]\n",
        "num_rentals_df = filtered_df.groupby([\"date\", \"dayofweek\", \"start_station_name\"]).size().reset_index(name=\"num_rentals\")\n",
        "\n",
        "# join with the weather data\n",
        "ready_for_ml = num_rentals_df.merge(weather_data_df, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
        "\n",
        "# 2. Preprocess the data\n",
        "ready_for_ml = ready_for_ml.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66NhL3dEBn1U"
      },
      "source": [
        "## Classic Scikit learn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYkhgMavOUeb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "training_data_pandas = ready_for_ml\n",
        "\n",
        "# Pick feature columns and label column\n",
        "# define features\n",
        "X = training_data_pandas[\n",
        "    [\n",
        "        \"dayofweek\",\n",
        "        \"start_station_name\",\n",
        "        \"precipitation\"\n",
        "    ]\n",
        "]\n",
        "# define target\n",
        "y = training_data_pandas[\"num_rentals\"]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create preprocessing transformers\n",
        "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
        "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "# Combine transformers using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[('num', numeric_transformer, ['precipitation']), ('cat', categorical_transformer, ['dayofweek', 'start_station_name'])\n",
        "])\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create and train the model\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                        ('regressor', LinearRegression(fit_intercept=False))])\n",
        "\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO4gnZ3gOwXG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntW0A-voQMEh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the minimum and maximum values from both y_test and y_pred\n",
        "min_val = min(min(y_test), min(y_pred))\n",
        "max_val = max(max(y_test), max(y_pred))\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.scatter(y_test, y_pred)\n",
        "\n",
        "# Set the x and y axis limits to be the same\n",
        "plt.xlim(min_val, max_val)\n",
        "plt.ylim(min_val, max_val)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Actual Values (y_test)\")\n",
        "plt.ylabel(\"Predicted Values (y_pred)\")\n",
        "plt.title(\"Actual vs. Predicted Values\")\n",
        "\n",
        "# Add a diagonal line for reference (optional)\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--')  # Dashed diagonal line\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24m_rPE1BSio"
      },
      "source": [
        "## Equivalent ML Model with BigFrames ML (BQML)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TqLnKcsv14W"
      },
      "outputs": [],
      "source": [
        "import bigframes.pandas as bpd\n",
        "from bigframes.ml.linear_model import LinearRegression\n",
        "\n",
        "# Split data into features (X) and target variable (y)\n",
        "# define features\n",
        "X = ready_for_ml[\n",
        "    [\n",
        "        \"dayofweek\",\n",
        "        \"start_station_name\",\n",
        "        \"precipitation\"\n",
        "    ]\n",
        "]\n",
        "# define target\n",
        "y = ready_for_ml[\"num_rentals\"]\n",
        "\n",
        "# Create and train the Linear Regression model\n",
        "model = LinearRegression(fit_intercept=False)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 5. Evaluate the model (optional)\n",
        "print(f\"R-squared: {model.score(X, y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5uLvVQCBeg5"
      },
      "source": [
        "Predict on a no rain day:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqsXIJxW14Oc"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# 6. Make predictions (example)\n",
        "# Create a new DataFrame with the same features for prediction\n",
        "no_rain_monday_day = bpd.DataFrame(\n",
        "    {\n",
        "        \"dayofweek\": 0,\n",
        "        \"start_station_name\": \"Black Lion Gate, Kensington Gardens\",\n",
        "        \"precipitation\": 0\n",
        "    },\n",
        "    index=[0]\n",
        ")\n",
        "\n",
        "predictions = model.predict(no_rain_monday_day)\n",
        "predictions.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opQ3XsDsBiEM"
      },
      "source": [
        "Predict on a heavy rain day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRPMyP3N4nSy"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# 6. Make predictions (example)\n",
        "# Create a new DataFrame with the same features for prediction\n",
        "heavy_rain_monday_day = bpd.DataFrame(\n",
        "    {\n",
        "        \"dayofweek\": 0,\n",
        "        \"start_station_name\": \"Black Lion Gate, Kensington Gardens\",\n",
        "        \"precipitation\": 1000\n",
        "    },\n",
        "    index=[0]\n",
        ")\n",
        "\n",
        "predictions = model.predict(heavy_rain_monday_day)\n",
        "predictions.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIB6zU0qCFt5"
      },
      "source": [
        "## TODO: Exercise 4: Improve the Model\n",
        " Improve the model by improving the features or switching to the more powerful regressor, check documentation\n",
        "\n",
        " https://cloud.google.com/python/docs/reference/bigframes/latest/bigframes.ml.ensemble\n",
        "\n",
        " Hint: add isWeekend\n",
        " Hint: use XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA2qj5L3H0Gh"
      },
      "outputs": [],
      "source": [
        "import bigframes.pandas as bpd\n",
        "from bigframes.ml.linear_model import LinearRegression\n",
        "# TODO: import XGBRegressor regressor\n",
        "\n",
        "# 1. Load the bike rentals data from BigQuery and prepare the features\n",
        "# Load the dataframe\n",
        "cycle_hire_bdf = bpd.read_gbq(\n",
        "    \"bigquery-public-data.london_bicycles.cycle_hire\", max_results=MAX_ROWS, use_cache=False, columns=[\"start_date\", \"start_station_name\", \"duration\"],  filters=[(\"duration\", \">\", 0)]\n",
        ")\n",
        "cycle_hire_df = cycle_hire_bdf.to_pandas()   # cannot join between US and EU, so use both dataframes in pandas, pay attention to the LOAD jobs\n",
        "cycle_hire_df['date'] = cycle_hire_df['start_date'].dt.date\n",
        "cycle_hire_df['hour'] = cycle_hire_df['start_date'].dt.hour\n",
        "cycle_hire_df['dayofweek'] = cycle_hire_df['start_date'].dt.dayofweek\n",
        "cycle_hire_df['month'] = cycle_hire_df['start_date'].dt.month\n",
        "cycle_hire_df['year'] = cycle_hire_df['start_date'].dt.year\n",
        "# TODO: add more features\n",
        "\n",
        "filtered_df = cycle_hire_df.loc[:, [\"year\", \"date\", \"dayofweek\", \"start_station_name\"]].loc[cycle_hire_df[\"year\"] == 2022]\n",
        "num_rentals_df = filtered_df.groupby([\"date\", \"dayofweek\", \"start_station_name\"]).size().reset_index(name=\"num_rentals\")\n",
        "\n",
        "# join with the weather data\n",
        "ready_for_ml = num_rentals_df.merge(weather_data_df, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
        "\n",
        "# 2. Preprocess the data\n",
        "ready_for_ml = ready_for_ml.dropna()\n",
        "\n",
        "# 3. Split data into features (X) and target variable (y)\n",
        "# define features\n",
        "X = ready_for_ml[\n",
        "    [\n",
        "        \"dayofweek\",\n",
        "        \"start_station_name\",\n",
        "        \"precipitation\"\n",
        "    ]\n",
        "]\n",
        "# define target\n",
        "y = ready_for_ml[\"num_rentals\"]\n",
        "\n",
        "# 4. Try different regressors\n",
        "regressors = {\n",
        "    \"Linear Regression\": LinearRegression(fit_intercept=False),\n",
        "    # TODO: use XGBRegressor regressor\n",
        "}\n",
        "\n",
        "for name, regressor in regressors.items():\n",
        "    # Create and train the model\n",
        "    model = Pipeline(steps=[('regressor', regressor)])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Evaluate the model\n",
        "    print(f\"{name}: R-squared: {model.score(X_test, y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sopfn648CVma"
      },
      "outputs": [],
      "source": [
        "# @title Solution\n",
        "import bigframes.pandas as bpd\n",
        "from bigframes.ml.linear_model import LinearRegression\n",
        "from bigframes.ml.ensemble import XGBRegressor\n",
        "\n",
        "# 1. Load the bike rentals data from BigQuery and prepare the features\n",
        "# Load the dataframe\n",
        "cycle_hire_bdf = bpd.read_gbq(\n",
        "    \"bigquery-public-data.london_bicycles.cycle_hire\", max_results=MAX_ROWS, use_cache=False, columns=[\"start_date\", \"start_station_name\", \"duration\"],  filters=[(\"duration\", \">\", 0)]\n",
        ")\n",
        "cycle_hire_df = cycle_hire_bdf.to_pandas()   # cannot join between US and EU, so use both dataframes in pandas, pay attention to the LOAD jobs\n",
        "cycle_hire_df['date'] = cycle_hire_df['start_date'].dt.date\n",
        "cycle_hire_df['hour'] = cycle_hire_df['start_date'].dt.hour\n",
        "cycle_hire_df['dayofweek'] = cycle_hire_df['start_date'].dt.dayofweek\n",
        "cycle_hire_df['isweekend'] = cycle_hire_df['start_date'].dt.dayofweek >= 5\n",
        "cycle_hire_df['month'] = cycle_hire_df['start_date'].dt.month\n",
        "cycle_hire_df['year'] = cycle_hire_df['start_date'].dt.year\n",
        "\n",
        "filtered_df = cycle_hire_df.loc[:, [\"year\", \"date\", \"dayofweek\", \"isweekend\", \"start_station_name\"]].loc[cycle_hire_df[\"year\"] == 2022]\n",
        "num_rentals_df = filtered_df.groupby([\"date\", \"dayofweek\", \"isweekend\", \"start_station_name\"]).size().reset_index(name=\"num_rentals\")\n",
        "\n",
        "# join with the weather data\n",
        "ready_for_ml = num_rentals_df.merge(weather_data_df, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
        "\n",
        "# 2. Preprocess the data\n",
        "ready_for_ml = ready_for_ml.dropna()\n",
        "\n",
        "# 3. Split data into features (X) and target variable (y)\n",
        "# define features\n",
        "X = ready_for_ml[\n",
        "    [\n",
        "        \"dayofweek\",\n",
        "        \"isweekend\",\n",
        "        \"start_station_name\",\n",
        "        \"precipitation\"\n",
        "    ]\n",
        "]\n",
        "# define target\n",
        "y = ready_for_ml[\"num_rentals\"]\n",
        "\n",
        "# 4. Try different regressors\n",
        "regressors = {\n",
        "    \"Linear Regression\": LinearRegression(fit_intercept=False),\n",
        "    \"XGB Regressor\": XGBRegressor()\n",
        "}\n",
        "\n",
        "for name, regressor in regressors.items():\n",
        "    # Create and train the model\n",
        "    model = Pipeline(steps=[('regressor', regressor)])\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Evaluate the model\n",
        "    print(f\"{name}: R-squared: {model.score(X_test, y_test)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "bq-bigframes",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
